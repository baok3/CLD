{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "import traceback\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Torch empty cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_convolutional_layer(model):\n",
    "    \"\"\"\n",
    "    Recursively find the last convolutional layer in the model\n",
    "    \"\"\"\n",
    "    last_conv_layer = None\n",
    "    \n",
    "    def _find_conv_layer(module):\n",
    "        nonlocal last_conv_layer\n",
    "        for name, child in module.named_children():\n",
    "            if isinstance(child, (torch.nn.Conv2d, torch.nn.modules.conv._ConvNd)):\n",
    "                last_conv_layer = child\n",
    "            _find_conv_layer(child)\n",
    "    \n",
    "    _find_conv_layer(model)\n",
    "    return last_conv_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_path, num_classes=5):\n",
    "    # Determine model architecture based on filename\n",
    "    filename = os.path.basename(model_path)\n",
    "    \n",
    "    # Mapping of model architectures\n",
    "    model_architectures = {\n",
    "        'convnext_tiny': models.convnext_tiny(weights=None),\n",
    "        'mobilenetv2': models.mobilenet_v2(weights=None),\n",
    "        'regnety_8gf': models.regnet_y_8gf(weights=None),\n",
    "        'resnet50': models.resnet50(weights=None),\n",
    "        'efficientnet': models.efficientnet_b0(weights=None)\n",
    "    }\n",
    "    \n",
    "    # Select model architecture\n",
    "    for arch_name, model_class in model_architectures.items():\n",
    "        if arch_name in filename.lower():\n",
    "            model = model_class\n",
    "            break\n",
    "    else:\n",
    "        raise ValueError(f\"Could not determine model architecture for {filename}\")\n",
    "    \n",
    "    # Modify classifier for custom number of classes\n",
    "    if 'convnext_tiny' in filename.lower():\n",
    "        model.classifier[2] = torch.nn.Linear(in_features=model.classifier[2].in_features, out_features=num_classes)\n",
    "    \n",
    "    elif 'mobilenetv2' in filename.lower():\n",
    "        model.classifier[1] = torch.nn.Linear(in_features=model.classifier[1].in_features, out_features=num_classes)\n",
    "    \n",
    "    elif 'regnety' in filename.lower():\n",
    "        model.fc = torch.nn.Linear(in_features=model.fc.in_features, out_features=num_classes)\n",
    "    \n",
    "    elif 'resnet50' in filename.lower():\n",
    "        model.fc = torch.nn.Linear(in_features=model.fc.in_features, out_features=num_classes)\n",
    "\n",
    "    elif 'efficientnet' in filename.lower():\n",
    "        model.classifier[1] = torch.nn.Linear(in_features=model.classifier[1].in_features, out_features=num_classes)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model architecture in {filename}\")\n",
    "    \n",
    "    # Load state dict\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint, strict=False)\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_layer(model, filename):\n",
    "    # Detailed handling for different model architectures\n",
    "    if 'convnext_tiny' in filename.lower():\n",
    "        # For ConvNeXt, use the last stage\n",
    "        return model.stages[-1]\n",
    "    \n",
    "    elif 'efficientnet' in filename.lower():\n",
    "        # For EfficientNet, use the last block in the network\n",
    "        return model.features[-1]\n",
    "    \n",
    "    elif 'mobilenetv2' in filename.lower():\n",
    "        # For MobileNetV2, use the last convolutional layer\n",
    "        return model.features[-1]\n",
    "    \n",
    "    elif 'regnety' in filename.lower():\n",
    "        # For RegNet, use the last block in the network\n",
    "        return model.trunk_output[-1]\n",
    "    \n",
    "    elif 'resnet50' in filename.lower():\n",
    "        # For ResNet, use the last convolutional layer of the last block\n",
    "        return model.layer4[-1]\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Could not determine target layer for {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, target_layer):\n",
    "        self.model = model.eval().to(device)\n",
    "        self.target_layer = target_layer.to(device)\n",
    "        self.activation = None\n",
    "        self.gradients = None\n",
    "\n",
    "        self.target_layer.register_forward_hook(self.forward_hook)\n",
    "        self.target_layer.register_full_backward_hook(self.backward_hook)\n",
    "\n",
    "\n",
    "    def forward_hook(self, module, input, output):\n",
    "        self.activation = output.detach()\n",
    "\n",
    "    def backward_hook(self, module, grad_in, grad_out):\n",
    "        if grad_out[0] is not None:\n",
    "            self.gradients = grad_out[0].detach()\n",
    "\n",
    "        else:\n",
    "            print(f\"Warning: backward_hook received None gradients for module {module}. Make sure the target layer is part of the computational graph for the loss.\")\n",
    "            self.gradients = None\n",
    "\n",
    "    def generate_heatmap(self, input_tensor,heatmap_method):\n",
    "        input_tensor = input_tensor.to(device)\n",
    "        if not input_tensor.requires_grad:\n",
    "            input_tensor.requires_grad_(True)\n",
    "\n",
    "        if not hasattr(self, 'forward_handle') or self.forward_handle is None:\n",
    "             self.forward_handle = self.target_layer.register_forward_hook(self.forward_hook)\n",
    "        if not hasattr(self, 'backward_handle') or self.backward_handle is None:\n",
    "             self.backward_handle = self.target_layer.register_full_backward_hook(self.backward_hook)\n",
    "        \n",
    "        self.activation = None\n",
    "        self.gradients = None\n",
    "        self.model.zero_grad()\n",
    "        \n",
    "        output = self.model(input_tensor)\n",
    "\n",
    "        target_class = output.argmax(dim=1).item()\n",
    "\n",
    "        if self.activation is None:\n",
    "            raise RuntimeError(\"Activation hook did not run. Check target_layer.\")\n",
    "\n",
    "        target_score = output[0, target_class]\n",
    "        target_score.backward(retain_graph=False, create_graph=False)\n",
    "        \n",
    "        if self.gradients is None:\n",
    "             raise RuntimeError(\"Backward hook did not capture gradients. Check target_layer involvement and backward pass.\")\n",
    "\n",
    "        if heatmap_method == 'default':\n",
    "            weights = self.gradients.mean(dim=(2, 3), keepdim=True) # Global Average Pooling of gradients\n",
    "            cam_raw = (weights * self.activation).sum(dim=1, keepdim=True) # Weighted sum of activations\n",
    "            cam_raw = torch.relu(cam_raw) # Apply ReLU\n",
    "            # Squeeze, move to CPU, convert to numpy AFTER ensuring it's not empty\n",
    "            if cam_raw.numel() > 0:\n",
    "                 cam = cam_raw.squeeze().cpu().numpy()\n",
    "            else:\n",
    "                 print(\"Warning: Raw CAM tensor is empty.\")\n",
    "                 # Handle appropriately, e.g., return zeros or raise error\n",
    "                 h, w = self.activation.shape[2:] # Get spatial dims from activation\n",
    "                 cam = np.zeros((h, w), dtype=np.float32)\n",
    "        \n",
    "        elif heatmap_method == 'guided':\n",
    "            weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
    "            cam_raw = (weights * self.activation).sum(dim=1, keepdim=True)\n",
    "            cam_raw = torch.relu(cam_raw)\n",
    "            if cam_raw.numel() > 0:\n",
    "                cam = cam_raw.squeeze().cpu().numpy()\n",
    "            else:\n",
    "                print(\"Warning: Raw CAM tensor is empty.\")\n",
    "                h, w = self.activation.shape[2:]\n",
    "                cam = np.zeros((h, w), dtype=np.float32)\n",
    "        \n",
    "        elif heatmap_method == 'gradcam++':\n",
    "            gradients_pow2 = self.gradients.pow(2)\n",
    "            gradients_pow3 = self.gradients.pow(3)\n",
    "            sum_act = self.activation.sum(dim=(2, 3), keepdim=True) # Sum over spatial dimensions\n",
    "\n",
    "            # Adding epsilon for numerical stability\n",
    "            eps = 1e-8\n",
    "            alpha_denom = 2.0 * gradients_pow2 + sum_act * gradients_pow3 + eps\n",
    "            alpha_num = gradients_pow2\n",
    "\n",
    "            # Element-wise division, handling potential division by zero via epsilon\n",
    "            alpha = alpha_num / alpha_denom\n",
    "\n",
    "            # Calculate weights: sum alpha * ReLU(gradients) over spatial dimensions\n",
    "            # Ensure gradients are positive using ReLU\n",
    "            weights = (alpha * torch.relu(self.gradients)).sum(dim=(2, 3), keepdim=True)\n",
    "\n",
    "            # Calculate CAM: sum weights * activations over the channel dimension\n",
    "            cam_raw = (weights * self.activation).sum(dim=1, keepdim=True)\n",
    "            cam_raw = torch.relu(cam_raw) # Apply ReLU to the final CAM\n",
    "\n",
    "            if cam_raw.numel() > 0:\n",
    "                 cam = cam_raw.squeeze().cpu().numpy()\n",
    "            else:\n",
    "                 print(\"Warning: Raw CAM tensor is empty.\")\n",
    "                 h, w = self.activation.shape[2:]\n",
    "                 cam = np.zeros((h, w), dtype=np.float32)\n",
    "\n",
    "        elif heatmap_method == 'augmented_gradcam++':\n",
    "            gradients_pow2 = self.gradients.pow(2)\n",
    "             # Denominator: 2 * grad^2 + sum(grad) - This looks unusual. Sum is usually over spatial dims.\n",
    "             # Let's assume sum over spatial dims as in GradCAM++ for consistency.\n",
    "            sum_grad_spatial = self.gradients.sum(dim=(2, 3), keepdim=True)\n",
    "            alpha_denom = 2.0 * gradients_pow2 + sum_grad_spatial + 1e-8 # Added epsilon\n",
    "            alpha_num = gradients_pow2\n",
    "            alpha = alpha_num / alpha_denom\n",
    "\n",
    "            weights = (alpha * torch.relu(self.gradients)).sum(dim=(2, 3), keepdim=True)\n",
    "\n",
    "            cam_raw = (weights * self.activation).sum(dim=1, keepdim=True)\n",
    "            # Missing ReLU on final CAM in original code snippet\n",
    "            cam_raw = torch.relu(cam_raw)\n",
    "\n",
    "            if cam_raw.numel() > 0:\n",
    "                cam = cam_raw.squeeze().cpu().numpy()\n",
    "            else:\n",
    "                print(\"Warning: Raw CAM tensor is empty.\")\n",
    "                h, w = self.activation.shape[2:]\n",
    "                cam = np.zeros((h, w), dtype=np.float32)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported method: {heatmap_method}\")\n",
    "\n",
    "        # Normalize\n",
    "        eps = 1e-8\n",
    "\n",
    "        if cam.max() > cam.min():\n",
    "            cam_normalized = (cam - cam.min()) / (cam.max() - cam.min() + eps)\n",
    "        elif cam.max() > 0: # Handle case where cam is constant but non-zero\n",
    "            cam_normalized = cam / cam.max() # Normalize to 0 or 1\n",
    "        else:\n",
    "            cam_normalized = cam\n",
    "\n",
    "        self.remove_hooks()\n",
    "\n",
    "        self.activation = None\n",
    "        self.gradients = None\n",
    "        if input_tensor.grad is not None:\n",
    "            input_tensor.grad.zero_() # Zero gradients on the input tensor\n",
    "\n",
    "\n",
    "        return cam_normalized\n",
    "\n",
    "    def remove_hooks(self):\n",
    "         \"\"\"Removes the forward and backward hooks.\"\"\"\n",
    "         if hasattr(self, 'forward_handle') and self.forward_handle:\n",
    "             self.forward_handle.remove()\n",
    "             self.forward_handle = None\n",
    "         if hasattr(self, 'backward_handle') and self.backward_handle:\n",
    "             self.backward_handle.remove()\n",
    "             self.backward_handle = None\n",
    "\n",
    "\n",
    "    def apply_heatmap(self, original_image, heatmap, alpha=0.4, colormap=cv2.COLORMAP_JET):\n",
    "        if heatmap is None or heatmap.size == 0:\n",
    "             print(\"Warning: Cannot apply empty heatmap.\")\n",
    "             return original_image\n",
    "\n",
    "        # Ensure original image is 8-bit\n",
    "        if original_image.dtype != np.uint8:\n",
    "            # Assuming original image pixels are in [0, 1] float range if not uint8\n",
    "            if original_image.max() <= 1.0:\n",
    "                original_image = (original_image * 255).astype(np.uint8)\n",
    "            else: # Otherwise, just try converting type, might need clipping\n",
    "                 original_image = original_image.astype(np.uint8)\n",
    "\n",
    "\n",
    "        h, w, _ = original_image.shape\n",
    "        # Ensure heatmap is float32 for resize, handle potential 1D heatmap\n",
    "        if heatmap.ndim == 1:\n",
    "             # Attempt to reshape if it's a flattened square, otherwise error\n",
    "             side = int(np.sqrt(heatmap.shape[0]))\n",
    "             if side * side == heatmap.shape[0]:\n",
    "                 heatmap = heatmap.reshape((side, side))\n",
    "             else:\n",
    "                 print(f\"Warning: Cannot reshape 1D heatmap of size {heatmap.shape[0]} into 2D.\")\n",
    "                 return original_image # Cannot proceed\n",
    "\n",
    "        # Resize heatmap\n",
    "        heatmap_resized = cv2.resize(heatmap.astype(np.float32), (w, h))\n",
    "\n",
    "        # Apply colormap - Ensure input to applyColorMap is uint8 [0, 255]\n",
    "        heatmap_colored = cv2.applyColorMap(np.uint8(255 * heatmap_resized), colormap)\n",
    "\n",
    "        # Blend original image with heatmap\n",
    "        overlaid_image = cv2.addWeighted(original_image, 1 - alpha, heatmap_colored, alpha, 0)\n",
    "\n",
    "        return overlaid_image\n",
    "\n",
    "    def apply_bounding_box(self, original_image, heatmap, threshold=0.5, min_contour_area=50, use_morph_close=True, kernel_size=5):\n",
    "        if heatmap is None or heatmap.size == 0:\n",
    "            print(\"Warning: Cannot apply bounding box to empty heatmap.\")\n",
    "            return original_image\n",
    "\n",
    "        # Ensure original image is 8-bit BGR\n",
    "        if original_image.dtype != np.uint8:\n",
    "             if original_image.max() <= 1.0: # Assume float [0, 1]\n",
    "                 image_uint8 = (original_image * 255).astype(np.uint8)\n",
    "             else: # Assume float [0, 255] or other scale, just convert\n",
    "                 image_uint8 = original_image.astype(np.uint8)\n",
    "        else:\n",
    "             image_uint8 = original_image.copy() # Work on a copy\n",
    "\n",
    "        # Ensure image is 3 channels for drawing colored rectangle\n",
    "        if image_uint8.ndim == 2:\n",
    "             image_uint8 = cv2.cvtColor(image_uint8, cv2.COLOR_GRAY2BGR)\n",
    "        elif image_uint8.shape[2] == 4: # Handle RGBA\n",
    "             image_uint8 = cv2.cvtColor(image_uint8, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "\n",
    "        h, w, _ = image_uint8.shape\n",
    "\n",
    "        # Resize heatmap and ensure it's float32\n",
    "        if heatmap.ndim == 1: # Handle flattened heatmap case\n",
    "             side = int(np.sqrt(heatmap.shape[0]))\n",
    "             if side * side == heatmap.shape[0]:\n",
    "                 heatmap = heatmap.reshape((side, side))\n",
    "             else:\n",
    "                 print(f\"Warning: Cannot reshape 1D heatmap of size {heatmap.shape[0]} for bounding box.\")\n",
    "                 return image_uint8\n",
    "\n",
    "        heatmap_resized = cv2.resize(heatmap.astype(np.float32), (w, h))\n",
    "\n",
    "        # Apply threshold to get binary map [0, 255]\n",
    "        # Ensure the input heatmap is scaled 0-1 before multiplying by 255\n",
    "        if heatmap_resized.max() > 1.0: # Check if already potentially [0, 255]\n",
    "             heatmap_norm = heatmap_resized / 255.0\n",
    "        else:\n",
    "             heatmap_norm = heatmap_resized\n",
    "\n",
    "        _, binary_map = cv2.threshold(np.uint8(255 * heatmap_norm), int(threshold * 255), 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        # Optional: Morphological closing to connect nearby regions\n",
    "        if use_morph_close:\n",
    "            kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "            binary_map = cv2.morphologyEx(binary_map, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Find contours\n",
    "        contours, _ = cv2.findContours(binary_map, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Variables to store the overall bounding box coordinates\n",
    "        overall_min_x, overall_min_y = w, h # Initialize with image dimensions\n",
    "        overall_max_x, overall_max_y = 0, 0\n",
    "        found_significant_contour = False\n",
    "\n",
    "        # Iterate through contours to find the overall bounding box\n",
    "        for contour in contours:\n",
    "            # Filter based on contour area\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area >= min_contour_area:\n",
    "                found_significant_contour = True\n",
    "                # Get bounding box for this contour\n",
    "                x, y, wb, hb = cv2.boundingRect(contour)\n",
    "                # Update overall coordinates\n",
    "                overall_min_x = min(overall_min_x, x)\n",
    "                overall_min_y = min(overall_min_y, y)\n",
    "                overall_max_x = max(overall_max_x, x + wb)\n",
    "                overall_max_y = max(overall_max_y, y + hb)\n",
    "\n",
    "        # Draw the single overall bounding box if any significant contours were found\n",
    "        if found_significant_contour:\n",
    "            cv2.rectangle(image_uint8, (overall_min_x, overall_min_y), (overall_max_x, overall_max_y), (0, 255, 0), 2) # Green box, thickness 2\n",
    "            # Optional: Add label or confidence if available\n",
    "            #  label = f\"Detection: Conf {confidence:.2f}\"\n",
    "            # cv2.putText(image_uint8, label, (overall_min_x, overall_min_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "        return image_uint8\n",
    "\n",
    "    # --- Alternative using connectedComponentsWithStats ---\n",
    "    def apply_bounding_box_connected_components(self, original_image, heatmap, threshold=0.5, min_component_area=50, use_morph_close=True, kernel_size=5):\n",
    "        \"\"\"\n",
    "        Alternative method using connectedComponentsWithStats to draw a single bounding box.\n",
    "        \"\"\"\n",
    "        if heatmap is None or heatmap.size == 0:\n",
    "             print(\"Warning: Cannot apply bounding box to empty heatmap.\")\n",
    "             return original_image\n",
    "\n",
    "        # Basic image prep (similar to the other method)\n",
    "        if original_image.dtype != np.uint8:\n",
    "             if original_image.max() <= 1.0: image_uint8 = (original_image * 255).astype(np.uint8)\n",
    "             else: image_uint8 = original_image.astype(np.uint8)\n",
    "        else: image_uint8 = original_image.copy()\n",
    "\n",
    "        if image_uint8.ndim == 2: image_uint8 = cv2.cvtColor(image_uint8, cv2.COLOR_GRAY2BGR)\n",
    "        elif image_uint8.shape[2] == 4: image_uint8 = cv2.cvtColor(image_uint8, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "        h, w, _ = image_uint8.shape\n",
    "\n",
    "        # Heatmap prep (similar to the other method)\n",
    "        if heatmap.ndim == 1:\n",
    "             side = int(np.sqrt(heatmap.shape[0]))\n",
    "             if side * side == heatmap.shape[0]: heatmap = heatmap.reshape((side, side))\n",
    "             else: return image_uint8\n",
    "        heatmap_resized = cv2.resize(heatmap.astype(np.float32), (w, h))\n",
    "        if heatmap_resized.max() > 1.0: heatmap_norm = heatmap_resized / 255.0\n",
    "        else: heatmap_norm = heatmap_resized\n",
    "        _, binary_map = cv2.threshold(np.uint8(255 * heatmap_norm), int(threshold * 255), 255, cv2.THRESH_BINARY)\n",
    "\n",
    "        if use_morph_close:\n",
    "            kernel = np.ones((kernel_size, kernel_size), np.uint8)\n",
    "            binary_map = cv2.morphologyEx(binary_map, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Find connected components\n",
    "        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(binary_map, connectivity=8)\n",
    "\n",
    "        # Variables for overall bounding box\n",
    "        overall_min_x, overall_min_y = w, h\n",
    "        overall_max_x, overall_max_y = 0, 0\n",
    "        found_significant_component = False\n",
    "\n",
    "        # Iterate through components (label 0 is the background)\n",
    "        for i in range(1, num_labels):\n",
    "            area = stats[i, cv2.CC_STAT_AREA]\n",
    "            if area >= min_component_area:\n",
    "                found_significant_component = True\n",
    "                x = stats[i, cv2.CC_STAT_LEFT]\n",
    "                y = stats[i, cv2.CC_STAT_TOP]\n",
    "                wb = stats[i, cv2.CC_STAT_WIDTH]\n",
    "                hb = stats[i, cv2.CC_STAT_HEIGHT]\n",
    "\n",
    "                # Update overall coordinates\n",
    "                overall_min_x = min(overall_min_x, x)\n",
    "                overall_min_y = min(overall_min_y, y)\n",
    "                overall_max_x = max(overall_max_x, x + wb)\n",
    "                overall_max_y = max(overall_max_y, y + hb)\n",
    "\n",
    "        # Draw the single overall bounding box\n",
    "        if found_significant_component:\n",
    "            cv2.rectangle(image_uint8, (overall_min_x, overall_min_y), (overall_max_x, overall_max_y), (0, 255, 0), 2)\n",
    "\n",
    "        return image_uint8\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gradcam(model_path, image_path, threshold, heatmap_method, alpha):\n",
    "    try:\n",
    "        # Load model\n",
    "        model = load_model(model_path)\n",
    "        \n",
    "        # Find target layer dynamically\n",
    "        target_layer = find_last_convolutional_layer(model)\n",
    "        \n",
    "        # Read and preprocess image\n",
    "        original_image = cv2.imread(image_path)\n",
    "        if original_image is None:\n",
    "            print(f\"Could not read image: {image_path}\")\n",
    "            return None\n",
    "\n",
    "        # Convert to RGB if needed\n",
    "        if len(original_image.shape) == 2 or original_image.shape[2] == 1:\n",
    "            original_image = cv2.cvtColor(original_image, cv2.COLOR_GRAY2RGB)\n",
    "        elif original_image.shape[2] == 4:\n",
    "            original_image = cv2.cvtColor(original_image, cv2.COLOR_RGBA2RGB)\n",
    "\n",
    "        # Prepare input tensor\n",
    "        input_tensor = transform(Image.fromarray(original_image)).unsqueeze(0)\n",
    "        \n",
    "        # Apply Grad-CAM\n",
    "        gradcam = GradCAM(model, target_layer)\n",
    "        heatmap = gradcam.generate_heatmap(input_tensor, heatmap_method)\n",
    "        if heatmap is None:\n",
    "             print(f\"Failed to generate heatmap for {image_path}\")\n",
    "             return None\n",
    "        overlay_image = gradcam.apply_heatmap(\n",
    "            original_image, \n",
    "            heatmap, \n",
    "            alpha=alpha\n",
    "        )\n",
    "        # Create overlay and bounding boxes\n",
    "        result_image = gradcam.apply_bounding_box(overlay_image, heatmap, threshold)\n",
    "        return result_image\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(model_paths, root_dir, output_base_dir, threshold=0.5, heatmap_method='default', alpha=0.4, max_images_per_class=20):\n",
    "    print(f\"--- Starting GradCam Process (Max {max_images_per_class} images per class) ---\")\n",
    "\n",
    "    for model_path in model_paths:\n",
    "        # Create output directory for this model\n",
    "        model_name = os.path.splitext(os.path.basename(model_path))[0]\n",
    "        output_dir = os.path.join(output_base_dir, model_name)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"\\nProcessing model: {model_name}\")\n",
    "        print(f\"Output directory: {output_dir}\")\n",
    "\n",
    "        # Iterate through class directories\n",
    "        for class_name in sorted(os.listdir(root_dir)): # Use sorted() for consistent order\n",
    "            class_path = os.path.join(root_dir, class_name)\n",
    "            if not os.path.isdir(class_path):\n",
    "                continue\n",
    "\n",
    "            output_class_dir = os.path.join(output_dir, class_name)\n",
    "            os.makedirs(output_class_dir, exist_ok=True)\n",
    "            print(f\"  Processing class: {class_name}\")\n",
    "\n",
    "            images_processed_in_class = 0 # Initialize counter for this specific class\n",
    "\n",
    "            # Iterate through images in the class directory\n",
    "            for img_name in sorted(os.listdir(class_path)): # Use sorted() for consistent order\n",
    "                # --- Check if the maximum image count for this CLASS has been reached ---\n",
    "                if images_processed_in_class >= max_images_per_class:\n",
    "                    print(f\"    Reached limit of {max_images_per_class} images for class {class_name}. Moving to next class.\")\n",
    "                    break # Exit the inner loop (image loop) for this class\n",
    "\n",
    "                # Skip files that don't start with 'orig_'\n",
    "                if not img_name.startswith(\"orig_\"):\n",
    "                    continue\n",
    "\n",
    "                img_path = os.path.join(class_path, img_name)\n",
    "                # Check if it's a valid image file\n",
    "                if not img_path.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "                    continue\n",
    "\n",
    "                # --- At this point, the image is eligible for processing ---\n",
    "\n",
    "                # --- Direct function call instead of submitting to executor ---\n",
    "                try:\n",
    "                    # print(f\"Applying Grad-CAM to: {img_path}\") # Optional: more verbose logging\n",
    "                    result_image = apply_gradcam(model_path, img_path, threshold, heatmap_method, alpha)\n",
    "\n",
    "                    # --- Process result immediately ---\n",
    "                    if result_image is not None:\n",
    "                        output_path = os.path.join(output_class_dir, img_name)\n",
    "                        cv2.imwrite(output_path, result_image)\n",
    "                        # Increment counter only after successful processing and saving attempt\n",
    "                        images_processed_in_class += 1\n",
    "                        print(f\"    Processed and saved: {output_path} ({images_processed_in_class}/{max_images_per_class})\")\n",
    "                    else:\n",
    "                         print(f\"    Skipped saving (result was None): {img_path}\")\n",
    "                         # Decide if a None result should count towards the limit.\n",
    "                         # If yes, uncomment the next line:\n",
    "                         # images_processed_in_class += 1\n",
    "\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {img_path}: {e}\")\n",
    "                    # Decide if an error should count towards the limit.\n",
    "                    # If yes, uncomment the next line:\n",
    "                    # images_processed_in_class += 1\n",
    "                    # continue # Continue to the next image even if one fails\n",
    "\n",
    "            # Optional: Indicate when a class folder is finished processing all its allowed images\n",
    "            if images_processed_in_class < max_images_per_class:\n",
    "                 print(f\"  Finished class {class_name} (processed {images_processed_in_class} images).\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Processing Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_paths = [\n",
    "    # \"../model/efficientnet_coffee.pth\",\n",
    "    # \"../model/convnext_tiny_coffee.pth\",\n",
    "    # \"../model/mobilenetv2_coffee.pth\",\n",
    "    # \"../model/regnety_8gf_coffee.pth\",\n",
    "    # \"../model/efficientnet_10shot_20episode.pth\",\n",
    "    # \"../model/efficientnet_fewshot_20_5.pth\",\n",
    "    #\"../model/resnet50_coffee.pth\",\n",
    "    \"../model/resnet50_coffee_no_au.pth\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold= 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting GradCam Process (Max 20 images per class) ---\n",
      "\n",
      "Processing model: resnet50_coffee_no_au\n",
      "Output directory: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\n",
      "  Processing class: Cerscospora\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\OS\\AppData\\Local\\Temp\\ipykernel_74896\\2729930924.py:42: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (100).jpg (1/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1001).jpg (2/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1002).jpg (3/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1003).jpg (4/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1004).jpg (5/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1005).jpg (6/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1006).jpg (7/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1007).jpg (8/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1008).jpg (9/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (101).jpg (10/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1010).jpg (11/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1012).jpg (12/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1013).jpg (13/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1014).jpg (14/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1015).jpg (15/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1016).jpg (16/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1017).jpg (17/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1018).jpg (18/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (1019).jpg (19/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Cerscospora\\orig_4 (102).jpg (20/20)\n",
      "    Reached limit of 20 images for class Cerscospora. Moving to next class.\n",
      "  Processing class: Healthy\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1).jpg (1/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (10).jpg (2/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (100).jpg (3/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1000).jpg (4/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1001).jpg (5/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1002).jpg (6/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1003).jpg (7/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1004).jpg (8/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1005).jpg (9/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1006).jpg (10/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1007).jpg (11/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1008).jpg (12/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1009).jpg (13/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (101).jpg (14/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1010).jpg (15/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1011).jpg (16/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1012).jpg (17/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1013).jpg (18/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1016).jpg (19/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Healthy\\orig_1 (1017).jpg (20/20)\n",
      "    Reached limit of 20 images for class Healthy. Moving to next class.\n",
      "  Processing class: Leaf rust\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_059eaf863cca.jpeg (1/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (1).jpg (2/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (100).jpg (3/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (101).jpg (4/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (103).jpg (5/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (104).jpg (6/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (106).jpg (7/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (107).jpg (8/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (108).jpg (9/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (109).jpg (10/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (114).jpg (11/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (115).jpg (12/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (116).jpg (13/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (118).jpg (14/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (12).jpg (15/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (121).jpg (16/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (122).jpg (17/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (124).jpg (18/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (126).jpg (19/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Leaf rust\\orig_1 (127).jpg (20/20)\n",
      "    Reached limit of 20 images for class Leaf rust. Moving to next class.\n",
      "  Processing class: Miner\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (1).jpg (1/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10000).jpg (2/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10001).jpg (3/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10002).jpg (4/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10004).jpg (5/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10005).jpg (6/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10006).jpg (7/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10008).jpg (8/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10009).jpg (9/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10010).jpg (10/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10011).jpg (11/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10012).jpg (12/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10013).jpg (13/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10014).jpg (14/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10015).jpg (15/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10016).jpg (16/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10017).jpg (17/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10018).jpg (18/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10019).jpg (19/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Miner\\orig_1 (10020).jpg (20/20)\n",
      "    Reached limit of 20 images for class Miner. Moving to next class.\n",
      "  Processing class: Phoma\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (1).jpg (1/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (101).jpg (2/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (102).jpg (3/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (105).jpg (4/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (106).jpg (5/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (107).jpg (6/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (109).jpg (7/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (11).jpg (8/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (112).jpg (9/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (113).jpg (10/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (117).jpg (11/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (119).jpg (12/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (12).jpg (13/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (122).jpg (14/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (123).jpg (15/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (124).jpg (16/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (125).jpg (17/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (128).jpg (18/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (129).jpg (19/20)\n",
      "    Processed and saved: ../data/bounding_box_gradcam_0.7_gradcam++_no_heatmap/train/resnet50_coffee_no_au\\Phoma\\orig_1 (13).jpg (20/20)\n",
      "    Reached limit of 20 images for class Phoma. Moving to next class.\n",
      "\n",
      "--- Processing Finished ---\n"
     ]
    }
   ],
   "source": [
    "for n, model_path in enumerate(model_paths):\n",
    "    output_path = f\"../data/bounding_box_gradcam_{threshold}_gradcam++_no_heatmap/train/\"\n",
    "    \n",
    "    process_dataset(\n",
    "        [model_path], \n",
    "        \"../data/Final_CLD_data/train/\", \n",
    "        output_path, \n",
    "        threshold=threshold,\n",
    "        heatmap_method='gradcam++',  # 'default', 'guided', 'gradcam++', 'augmented_gradcam++'\n",
    "        alpha=0.0\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "few_shot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
