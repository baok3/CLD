{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56418aa",
   "metadata": {},
   "source": [
    "### Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d9cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision.ops import RoIAlign\n",
    "import torchvision.transforms.v2 as T\n",
    "from torchvision.transforms.v2 import functional as F, Transform, Compose, ToDtype, Normalize\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torchmetrics\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30faed62",
   "metadata": {},
   "source": [
    "### Data Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "169e4443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "DATA_PATH = './data/rare_fewshot_detection/'\n",
    "TRAIN_IMG_DIR = os.path.join(DATA_PATH, 'train/images')\n",
    "TRAIN_LABEL_DIR = os.path.join(DATA_PATH, 'train/labels')\n",
    "TEST_IMG_DIR = os.path.join(DATA_PATH, 'test/images')\n",
    "TEST_LABEL_DIR = os.path.join(DATA_PATH, 'test/labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f8af096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 7\n"
     ]
    }
   ],
   "source": [
    "N_SHOTS = 5\n",
    "CLASS_NAMES = ['cerscospora', 'healthy', 'leaf rust','miner' ,'phoma','nematode', 'pink disease']\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "print(f\"Number of classes: {NUM_CLASSES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e707a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class map: {'cerscospora': 1, 'healthy': 2, 'leaf rust': 3, 'miner': 4, 'phoma': 5, 'nematode': 6, 'pink disease': 7}\n",
      "Inverse class map: {1: 'cerscospora', 2: 'healthy', 3: 'leaf rust', 4: 'miner', 5: 'phoma', 6: 'nematode', 7: 'pink disease'}\n"
     ]
    }
   ],
   "source": [
    "BACKGROUND_CLASS_ID = 0\n",
    "CLASS_MAP = {name: i + 1 for i, name in enumerate(CLASS_NAMES)}\n",
    "INV_CLASS_MAP = {v: k for k, v in CLASS_MAP.items()}\n",
    "print(f\"Inverse class map: {INV_CLASS_MAP}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e29194b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Class IDs: {1, 2, 3, 4, 5, 6, 7}\n"
     ]
    }
   ],
   "source": [
    "TARGET_CLASS_IDS = set(CLASS_MAP.values()) # Should be {0, 1, 2, 3, 4, 5, 6}\n",
    "print(f\"Target Class IDs: {TARGET_CLASS_IDS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7241dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "YOLO_MODEL_PATH = '../object_detection_full_data/runs/detect/train/best.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b741b03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURE_LAYER_INDEX = -4\n",
    "ROI_OUTPUT_SIZE = (7, 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5280be45",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad4e03f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 10\n",
    "LEARNING_RATE = 1e-5\n",
    "WEIGHT_DECAY = 0.0005\n",
    "BATCH_SIZE = 4\n",
    "VALIDATION_SPLIT = 0.2\n",
    "PROTO_UPDATE_FREQ = 5\n",
    "FINETUNE_FEATURE_EXTRACTOR = True\n",
    "SAVE_PROTOTYPES_PATH = './models/yolov11n_fewshot_prototypes.pt'\n",
    "SAVE_FINETUNED_MODEL_PATH = './models/yolov11n_fewshot_finetuned.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42226b95",
   "metadata": {},
   "source": [
    "### Data Augmentation / Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a593359",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SquarePad(Transform):\n",
    "    def _transform(self, inpt, params):\n",
    "        img = inpt\n",
    "        _, h, w = img.shape\n",
    "        max_wh = max(w, h)\n",
    "        p_left, p_top = [(max_wh - s) // 2 for s in (w, h)]\n",
    "        p_right, p_bottom = [max_wh - s - p for s, p in zip((w, h), (p_left, p_top))]\n",
    "        padding = (p_left, p_top, p_right, p_bottom)\n",
    "        return F.pad(img, padding, fill=114/255.0)\n",
    "\n",
    "def get_transform(target_size=(640, 640)):\n",
    "    transforms = []\n",
    "    # transforms.append(SquarePad()) # Optional, depends on YOLO model needs\n",
    "    transforms.append(T.Resize(target_size, antialias=True))\n",
    "    transforms.append(T.ToDtype(torch.float32, scale=True))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c53ea",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc071b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RareDiseaseDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, target_class_ids, transforms=None, target_img_size=(640, 640)):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.target_class_ids = target_class_ids\n",
    "        self.transforms = transforms\n",
    "        self.target_img_size = target_img_size\n",
    "\n",
    "        self.image_files = sorted([f for f in os.listdir(img_dir) if f.endswith(('.jpg', '.png', '.jpeg'))])\n",
    "        self.img_ids = [os.path.splitext(f)[0] for f in self.image_files]\n",
    "\n",
    "        print(\"=\"*30)\n",
    "        print(\"Dataset Initialized:\")\n",
    "        print(f\"Image directory: {self.img_dir}\")\n",
    "        print(f\"Label directory: {self.label_dir} (Expecting YOLO .txt format)\")\n",
    "        print(f\"Target Few-Shot Class IDs: {self.target_class_ids}\")\n",
    "        print(\"IMPORTANT: Assumes class IDs in .txt files directly correspond to the target IDs.\")\n",
    "        print(\"           Ensure sufficient examples exist for N-shot learning per class.\")\n",
    "        print(\"=\"*30)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.img_ids[idx]\n",
    "        img_path = os.path.join(self.img_dir, self.image_files[idx])\n",
    "        label_path = os.path.join(self.label_dir, f\"{img_id}.txt\")\n",
    "\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "            original_w, original_h = img.size\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: Image file not found at {img_path}\")\n",
    "            return None, None # Handle appropriately later\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        try:\n",
    "            with open(label_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) != 5: continue\n",
    "                    class_id = int(parts[0])\n",
    "\n",
    "                    # Filter: Keep only target few-shot classes\n",
    "                    if class_id not in self.target_class_ids:\n",
    "                        continue\n",
    "\n",
    "                    xc, yc, w, h = map(float, parts[1:])\n",
    "                    xmin = (xc - w / 2) * original_w\n",
    "                    ymin = (yc - h / 2) * original_h\n",
    "                    xmax = (xc + w / 2) * original_w\n",
    "                    ymax = (yc + h / 2) * original_h\n",
    "\n",
    "                    xmin, ymin = max(0, xmin), max(0, ymin)\n",
    "                    xmax, ymax = min(original_w, xmax), min(original_h, ymax)\n",
    "\n",
    "                    if xmax <= xmin or ymax <= ymin: continue\n",
    "                    boxes.append([xmin, ymin, xmax, ymax])\n",
    "                    labels.append(class_id) # Use the target ID (0-6)\n",
    "\n",
    "        except FileNotFoundError: pass\n",
    "        except Exception as e: print(f\"Warning: Error reading {label_path}: {e}\")\n",
    "\n",
    "        if not boxes:\n",
    "            boxes_tensor = torch.zeros((0, 4), dtype=torch.float32)\n",
    "            labels_tensor = torch.zeros((0,), dtype=torch.int64)\n",
    "        else:\n",
    "            boxes_tensor = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "            labels_tensor = torch.as_tensor(labels, dtype=torch.int64)\n",
    "\n",
    "        target = {\"boxes\": boxes_tensor, \"labels\": labels_tensor, # Labels are 0-6\n",
    "                  \"image_id\": torch.tensor([idx]), \"original_size\": torch.tensor([original_w, original_h])}\n",
    "\n",
    "        if self.transforms:\n",
    "            img_tensor = self.transforms(img)\n",
    "            transformed_h, transformed_w = img_tensor.shape[1:]\n",
    "            scale_w = transformed_w / original_w\n",
    "            scale_h = transformed_h / original_h\n",
    "            target[\"scale_factor\"] = torch.tensor([scale_w, scale_h])\n",
    "        else:\n",
    "            img_tensor = T.functional.to_tensor(img)\n",
    "            target[\"scale_factor\"] = torch.tensor([1.0, 1.0])\n",
    "\n",
    "        return img_tensor, target\n",
    "\n",
    "    def get_image_ids_by_class(self):\n",
    "        \"\"\" Maps image indices to the few-shot class IDs they contain. \"\"\"\n",
    "        class_to_image_idxs = {cls_id: [] for cls_id in self.target_class_ids}\n",
    "        print(\"Mapping images to classes (using .txt labels)...\")\n",
    "        # It's more efficient to read labels once if dataset is large, but for small datasets getitem is fine.\n",
    "        for idx in tqdm(range(len(self))):\n",
    "            _, target = self.__getitem__(idx)\n",
    "            if target is None or target['labels'].numel() == 0: continue\n",
    "            unique_labels = torch.unique(target['labels'])\n",
    "            for label in unique_labels:\n",
    "                class_id = label.item()\n",
    "                if class_id in class_to_image_idxs: # Check if it's one of our target classes\n",
    "                    class_to_image_idxs[class_id].append(idx)\n",
    "        print(\"Mapping complete.\")\n",
    "        insufficient_classes = []\n",
    "        for class_id in self.target_class_ids:\n",
    "             class_name = INV_CLASS_MAP.get(class_id, f\"ID_{class_id}\")\n",
    "             count = len(class_to_image_idxs.get(class_id, []))\n",
    "             print(f\"  Class '{class_name}' (ID: {class_id}) found in {count} images.\")\n",
    "             if count < N_SHOTS:\n",
    "                 print(f\"ERROR: Class '{class_name}' needs {N_SHOTS} images for support set, but only found {count}.\")\n",
    "                 insufficient_classes.append(class_name)\n",
    "        if insufficient_classes:\n",
    "             raise ValueError(f\"Insufficient images for N-shot learning for classes: {', '.join(insufficient_classes)}\")\n",
    "        return class_to_image_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a40b20",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc2eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch = list(filter(lambda x: x is not None and x[0] is not None and x[1] is not None, batch))\n",
    "    if not batch: return None, None\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def img_coords_to_feature_coords(boxes, img_shape, feature_map_shape):\n",
    "    img_h, img_w = img_shape[-2:]\n",
    "    feat_h, feat_w = feature_map_shape[-2:]\n",
    "    scale_w = feat_w / img_w if img_w > 0 else 0\n",
    "    scale_h = feat_h / img_h if img_h > 0 else 0\n",
    "    scale_w = max(scale_w, 1e-6)\n",
    "    scale_h = max(scale_h, 1e-6)\n",
    "    scaled_boxes = boxes * torch.tensor([scale_w, scale_h, scale_w, scale_h], device=boxes.device)\n",
    "    scaled_boxes[:, 0::2] = torch.clamp(scaled_boxes[:, 0::2], 0, feat_w)\n",
    "    scaled_boxes[:, 1::2] = torch.clamp(scaled_boxes[:, 1::2], 0, feat_h)\n",
    "    return scaled_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b058dd5",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535e5a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YOLOFeatureExtractor(torch.nn.Module):\n",
    "    # (Implementation is the same as the previous response)\n",
    "    def __init__(self, yolo_model, feature_layer_index):\n",
    "        super().__init__()\n",
    "        self.yolo_model = yolo_model\n",
    "        try:\n",
    "            self.model_sequence = self.yolo_model.model.model\n",
    "        except AttributeError:\n",
    "            print(\"Error: Could not access `model.model` attribute in the loaded YOLO object.\")\n",
    "            print(\"       The model structure might be different. Try inspecting `yolo_model` directly.\")\n",
    "            raise\n",
    "        self.feature_layer_index = feature_layer_index\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = x\n",
    "        target_layer = self.feature_layer_index\n",
    "        if target_layer < 0:\n",
    "            if not self.model_sequence:\n",
    "                 raise ValueError(\"Model sequence not initialized\")\n",
    "            target_layer = len(self.model_sequence) + target_layer\n",
    "\n",
    "        try:\n",
    "            current_feature = features\n",
    "            for i, module in enumerate(self.model_sequence):\n",
    "                 current_feature = module(current_feature)\n",
    "                 if i == target_layer:\n",
    "                     features = current_feature\n",
    "                     break\n",
    "            else: # If loop finishes without break (target_layer invalid)\n",
    "                 if target_layer >= len(self.model_sequence):\n",
    "                     raise IndexError(f\"Feature layer index {self.feature_layer_index} (->{target_layer}) is out of bounds for sequence length {len(self.model_sequence)}\")\n",
    "                 # This else might be reachable if target_layer is the *last* layer index\n",
    "                 # In that case, features should already hold the output of the last layer.\n",
    "                 # Check if features were updated correctly.\n",
    "                 if i == target_layer:\n",
    "                     features = current_feature\n",
    "\n",
    "\n",
    "            # Handle potential list output from FPN/Concat layers\n",
    "            if isinstance(features, (list, tuple)):\n",
    "                 features = features[-1] # Take the last feature map (common heuristic)\n",
    "\n",
    "        except IndexError as e:\n",
    "            print(f\"ERROR accessing feature layer {self.feature_layer_index} (->{target_layer}): {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR during feature extraction at layer {target_layer}: {e}\")\n",
    "            print(f\"Input shape: {x.shape}\")\n",
    "            # print(\"Model sequence:\", self.model_sequence) # Can be very long\n",
    "            raise\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0b0768",
   "metadata": {},
   "source": [
    "### Prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669adf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prototypes(support_dataloader, feature_extractor, roi_align, target_class_ids, device):\n",
    "    # (Implementation is the same as the previous response, uses target_class_ids)\n",
    "    feature_extractor.eval()\n",
    "    prototypes = {cls_id: [] for cls_id in target_class_ids}\n",
    "    print(\"Calculating prototypes...\")\n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(support_dataloader, desc=\"Processing Support Set\"):\n",
    "            if images is None or targets is None: continue\n",
    "            try: images = torch.stack(images).to(device)\n",
    "            except Exception as e: print(f\"Skipping batch due to image stacking error: {e}\"); continue\n",
    "\n",
    "            feature_maps = feature_extractor(images)\n",
    "            if feature_maps is None: print(\"Feature extractor returned None\"); continue\n",
    "\n",
    "            for i in range(len(targets)):\n",
    "                target = targets[i]\n",
    "                gt_boxes = target['boxes'].to(device)\n",
    "                gt_labels = target['labels'].to(device) # Should be 0-6\n",
    "                if gt_boxes.shape[0] == 0: continue\n",
    "\n",
    "                img_shape = images[i].shape\n",
    "                # Handle case where feature map might have different batch dim if extractor behaves unexpectedly\n",
    "                fm_idx = i if feature_maps.shape[0] == images.shape[0] else 0\n",
    "                if fm_idx >= feature_maps.shape[0]: continue # Skip if feature map batch dim doesn't match\n",
    "                feature_map_shape = feature_maps[fm_idx].shape\n",
    "\n",
    "                feature_boxes = img_coords_to_feature_coords(gt_boxes, img_shape, feature_map_shape)\n",
    "                valid_box_indices = torch.where((feature_boxes[:, 2] > feature_boxes[:, 0]) & (feature_boxes[:, 3] > feature_boxes[:, 1]))[0]\n",
    "                if len(valid_box_indices) == 0: continue\n",
    "                feature_boxes = feature_boxes[valid_box_indices]\n",
    "                gt_labels = gt_labels[valid_box_indices]\n",
    "\n",
    "                roi_boxes = torch.cat([torch.zeros(feature_boxes.shape[0], 1, device=device), feature_boxes], dim=1)\n",
    "                try:\n",
    "                    # Ensure feature map used for RoIAlign has batch dim 1\n",
    "                    roi_features = roi_align(feature_maps[fm_idx].unsqueeze(0), [roi_boxes])\n",
    "                except Exception as e:\n",
    "                     print(f\"Error during RoIAlign: {e}\")\n",
    "                     print(f\"Feature map shape: {feature_maps[fm_idx].unsqueeze(0).shape}, RoI boxes shape: {roi_boxes.shape}\")\n",
    "                     continue\n",
    "\n",
    "                roi_features_flat = roi_features.view(roi_features.shape[0], -1)\n",
    "\n",
    "                for j, label in enumerate(gt_labels):\n",
    "                    class_id = label.item()\n",
    "                    if class_id in prototypes:\n",
    "                        prototypes[class_id].append(roi_features_flat[j].cpu())\n",
    "\n",
    "    final_prototypes = {}\n",
    "    print(\"Averaging features for final prototypes...\")\n",
    "    for class_id, features_list in prototypes.items():\n",
    "        if not features_list:\n",
    "            print(f\"Warning: No support features found for class ID {class_id}.\")\n",
    "            continue\n",
    "        try:\n",
    "             features_tensor = torch.stack(features_list)\n",
    "             final_prototypes[class_id] = torch.mean(features_tensor, dim=0)\n",
    "             print(f\"  Class ID {class_id}: {final_prototypes[class_id].shape}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing features for class {class_id}: {e}\")\n",
    "\n",
    "    if len(final_prototypes) != len(target_class_ids):\n",
    "         missing_ids = target_class_ids - set(final_prototypes.keys())\n",
    "         print(f\"ERROR: Failed to create prototypes for all target classes. Missing IDs: {missing_ids}\")\n",
    "         # Decide action: raise error or continue with available?\n",
    "         raise ValueError(\"Failed to create all prototypes\")\n",
    "\n",
    "    return final_prototypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e62d160",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bd5f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_few_shot_yolo(feature_extractor, prototypes, optimizer, scheduler, query_dataloader, roi_align, device, epoch, finetune_extractor=True):\n",
    "    if finetune_extractor: feature_extractor.train()\n",
    "    else: feature_extractor.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    batch_count = 0\n",
    "    progress_bar = tqdm(query_dataloader, desc=f\"Epoch {epoch+1} Training\", leave=False)\n",
    "\n",
    "    if not prototypes: print(\"Error: Prototypes dict empty. Skipping training.\"); return 0.0\n",
    "    proto_class_ids = sorted(list(prototypes.keys())) # e.g., [0, 1, 2, 3, 4, 5, 6]\n",
    "    if not proto_class_ids: print(\"Error: No valid class IDs in prototypes. Skipping training.\"); return 0.0\n",
    "\n",
    "    try: proto_tensor = torch.stack([prototypes[cid].to(device) for cid in proto_class_ids])\n",
    "    except Exception as e: print(f\"Error stacking proto tensors: {e}\"); return 0.0\n",
    "    class_id_to_proto_idx = {cls_id: idx for idx, cls_id in enumerate(proto_class_ids)}\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for images, targets in progress_bar:\n",
    "        if images is None or targets is None: continue\n",
    "        try: images = torch.stack(images).to(device)\n",
    "        except Exception as e: print(f\"Skipping batch due to image stacking error: {e}\"); continue\n",
    "\n",
    "        feature_maps = feature_extractor(images)\n",
    "        if feature_maps is None: print(\"Feature extractor returned None\"); continue\n",
    "\n",
    "        batch_loss = 0.0\n",
    "        valid_targets_in_batch = 0\n",
    "\n",
    "        for i in range(len(targets)):\n",
    "            target = targets[i]\n",
    "            gt_boxes = target['boxes'].to(device)\n",
    "            gt_labels = target['labels'].to(device) # Should be 0-6\n",
    "            if gt_boxes.shape[0] == 0: continue\n",
    "\n",
    "            # Filter GT labels for those we have prototypes for\n",
    "            valid_indices = [idx for idx, label in enumerate(gt_labels) if label.item() in prototypes]\n",
    "            if not valid_indices: continue\n",
    "            gt_boxes = gt_boxes[valid_indices]\n",
    "            gt_labels = gt_labels[valid_indices]\n",
    "\n",
    "            img_shape = images[i].shape\n",
    "            fm_idx = i if feature_maps.shape[0] == images.shape[0] else 0\n",
    "            if fm_idx >= feature_maps.shape[0]: continue\n",
    "            feature_map_shape = feature_maps[fm_idx].shape\n",
    "\n",
    "            feature_boxes = img_coords_to_feature_coords(gt_boxes, img_shape, feature_map_shape)\n",
    "            valid_box_indices = torch.where((feature_boxes[:, 2] > feature_boxes[:, 0]) & (feature_boxes[:, 3] > feature_boxes[:, 1]))[0]\n",
    "            if len(valid_box_indices) == 0: continue\n",
    "            feature_boxes = feature_boxes[valid_box_indices]\n",
    "            gt_labels = gt_labels[valid_box_indices]\n",
    "\n",
    "            roi_boxes = torch.cat([torch.zeros(feature_boxes.shape[0], 1, device=device), feature_boxes], dim=1)\n",
    "\n",
    "            try:\n",
    "                fm_input = feature_maps[fm_idx].unsqueeze(0)\n",
    "                if finetune_extractor: roi_features = roi_align(fm_input, [roi_boxes])\n",
    "                else:\n",
    "                    with torch.no_grad(): roi_features = roi_align(fm_input, [roi_boxes])\n",
    "            except Exception as e:\n",
    "                print(f\"Error during RoIAlign in training: {e}\")\n",
    "                print(f\"Feature map shape: {fm_input.shape}, RoI boxes shape: {roi_boxes.shape}\")\n",
    "                continue\n",
    "\n",
    "            roi_features_flat = roi_features.view(roi_features.shape[0], -1)\n",
    "\n",
    "            # Check feature dimension consistency\n",
    "            if proto_tensor.shape[1] != roi_features_flat.shape[1]:\n",
    "                 print(f\"WARNING: Feature dimension mismatch! Proto: {proto_tensor.shape[1]}, RoI: {roi_features_flat.shape[1]}. Skipping batch.\")\n",
    "                 # This often indicates the FEATURE_LAYER_INDEX is wrong or features changed unexpectedly.\n",
    "                 continue\n",
    "\n",
    "            roi_features_norm = torch.nn.functional.normalize(roi_features_flat, p=2, dim=1)\n",
    "            proto_tensor_norm = torch.nn.functional.normalize(proto_tensor, p=2, dim=1)\n",
    "            similarities = torch.mm(roi_features_norm, proto_tensor_norm.t())\n",
    "\n",
    "            try: target_proto_indices = torch.tensor([class_id_to_proto_idx[label.item()] for label in gt_labels], device=device, dtype=torch.long)\n",
    "            except KeyError as e: print(f\"Error: GT label {e} not in proto map. Skipping loss calc.\"); continue\n",
    "\n",
    "            loss = criterion(similarities, target_proto_indices)\n",
    "\n",
    "            if finetune_extractor and torch.isfinite(loss):\n",
    "                batch_loss += loss\n",
    "                valid_targets_in_batch += 1\n",
    "            elif not torch.isfinite(loss): print(f\"Warning: Non-finite loss: {loss.item()}. Skipping.\")\n",
    "\n",
    "        if finetune_extractor and valid_targets_in_batch > 0:\n",
    "             avg_batch_loss = batch_loss / valid_targets_in_batch\n",
    "             if torch.isfinite(avg_batch_loss):\n",
    "                  optimizer.zero_grad()\n",
    "                  avg_batch_loss.backward()\n",
    "                  optimizer.step()\n",
    "                  total_loss += avg_batch_loss.item()\n",
    "                  batch_count += 1\n",
    "                  progress_bar.set_postfix(loss=avg_batch_loss.item())\n",
    "             else: print(f\"Warning: Avg batch loss non-finite: {avg_batch_loss}. Skipping update.\")\n",
    "\n",
    "    if scheduler: scheduler.step()\n",
    "    avg_epoch_loss = total_loss / batch_count if batch_count > 0 else 0\n",
    "    if avg_epoch_loss > 0 or epoch == 0: print(f\"Epoch {epoch+1} Training Summary: Fine-tuning Loss: {avg_epoch_loss:.4f}\")\n",
    "    return avg_epoch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c1f012",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bedf45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def inference_yolo_few_shot(full_yolo_model, feature_extractor, prototypes, dataloader, roi_align, device, conf_threshold=0.3, nms_threshold=0.45):\n",
    "    full_yolo_model.eval()\n",
    "    feature_extractor.eval()\n",
    "\n",
    "    if not prototypes: print(\"Error: Prototypes dict empty for inference.\"); return [], []\n",
    "    proto_class_ids = sorted(list(prototypes.keys())) # e.g., [0, 1, ..., 6]\n",
    "    if not proto_class_ids: print(\"Error: No class IDs in prototypes for inference.\"); return [], []\n",
    "\n",
    "    try: proto_tensor = torch.stack([prototypes[cid].to(device) for cid in proto_class_ids])\n",
    "    except Exception as e: print(f\"Error stacking proto tensors for inference: {e}\"); return [], []\n",
    "    proto_idx_to_class_id = {idx: cls_id for idx, cls_id in enumerate(proto_class_ids)}\n",
    "\n",
    "    all_preds, all_targets_for_metric = [], []\n",
    "    print(\"Running inference with prototype re-classification...\")\n",
    "    progress_bar = tqdm(dataloader, desc=\"Inference\")\n",
    "\n",
    "    for images, targets in progress_bar:\n",
    "        if images is None or targets is None: continue\n",
    "        try: image_tensors = torch.stack(images).to(device)\n",
    "        except Exception as e: print(f\"Skipping batch due to image stacking error: {e}\"); continue\n",
    "\n",
    "        feature_maps = feature_extractor(image_tensors)\n",
    "        if feature_maps is None: print(\"Feature extractor returned None in inference\"); continue\n",
    "\n",
    "        # Use lower confidence for initial proposals, verbose=False reduces spam\n",
    "        yolo_results = full_yolo_model.predict(image_tensors, verbose=False, conf=0.05)\n",
    "\n",
    "        batch_preds = []\n",
    "        for i, result in enumerate(yolo_results):\n",
    "            img_shape = image_tensors[i].shape\n",
    "            fm_idx = i if feature_maps.shape[0] == image_tensors.shape[0] else 0\n",
    "            if fm_idx >= feature_maps.shape[0]: continue\n",
    "            feature_map_shape = feature_maps[fm_idx].shape\n",
    "\n",
    "            if result.boxes is None or len(result.boxes) == 0: proposal_boxes = torch.empty((0, 4), device=device)\n",
    "            else: proposal_boxes = result.boxes.xyxy.to(device)\n",
    "\n",
    "            if proposal_boxes.shape[0] == 0: # Handle no proposals case\n",
    "                 batch_preds.append({'boxes': torch.empty((0, 4), device=device), 'scores': torch.empty((0,), device=device), 'labels': torch.empty((0,), dtype=torch.int64, device=device)})\n",
    "                 continue\n",
    "\n",
    "            feature_boxes = img_coords_to_feature_coords(proposal_boxes, img_shape, feature_map_shape)\n",
    "            valid_box_indices = torch.where((feature_boxes[:, 2] > feature_boxes[:, 0]) & (feature_boxes[:, 3] > feature_boxes[:, 1]))[0]\n",
    "            if len(valid_box_indices) == 0: # Handle no valid feature boxes case\n",
    "                 batch_preds.append({'boxes': torch.empty((0, 4), device=device), 'scores': torch.empty((0,), device=device), 'labels': torch.empty((0,), dtype=torch.int64, device=device)})\n",
    "                 continue\n",
    "            feature_boxes = feature_boxes[valid_box_indices]\n",
    "            original_indices_kept = valid_box_indices\n",
    "\n",
    "            roi_boxes = torch.cat([torch.zeros(feature_boxes.shape[0], 1, device=device), feature_boxes], dim=1)\n",
    "            try:\n",
    "                fm_input = feature_maps[fm_idx].unsqueeze(0)\n",
    "                roi_features = roi_align(fm_input, [roi_boxes])\n",
    "            except Exception as e:\n",
    "                 print(f\"Error during RoIAlign in inference: {e}\")\n",
    "                 batch_preds.append({'boxes': torch.empty((0, 4), device=device), 'scores': torch.empty((0,), device=device), 'labels': torch.empty((0,), dtype=torch.int64, device=device)})\n",
    "                 continue # Skip to next image in batch\n",
    "\n",
    "\n",
    "            roi_features_flat = roi_features.view(roi_features.shape[0], -1)\n",
    "\n",
    "            if proto_tensor.shape[1] != roi_features_flat.shape[1]:\n",
    "                 print(f\"WARNING: Feature dimension mismatch inference! Proto: {proto_tensor.shape[1]}, RoI: {roi_features_flat.shape[1]}.\")\n",
    "                 batch_preds.append({'boxes': torch.empty((0, 4), device=device), 'scores': torch.empty((0,), device=device), 'labels': torch.empty((0,), dtype=torch.int64, device=device)})\n",
    "                 continue\n",
    "\n",
    "            roi_features_norm = torch.nn.functional.normalize(roi_features_flat, p=2, dim=1)\n",
    "            proto_tensor_norm = torch.nn.functional.normalize(proto_tensor, p=2, dim=1)\n",
    "            similarities = torch.mm(roi_features_norm, proto_tensor_norm.t())\n",
    "\n",
    "            new_scores, proto_indices = torch.max(similarities, dim=1)\n",
    "            # Map index (0..6) back to actual class ID (0..6)\n",
    "            new_labels = torch.tensor([proto_idx_to_class_id[idx.item()] for idx in proto_indices], device=device)\n",
    "\n",
    "            keep_indices = new_scores > conf_threshold\n",
    "            final_boxes = proposal_boxes[original_indices_kept][keep_indices]\n",
    "            final_scores = new_scores[keep_indices]\n",
    "            final_labels = new_labels[keep_indices]\n",
    "\n",
    "            if final_boxes.shape[0] > 0: # Only apply NMS if boxes remain\n",
    "                 nms_indices = torchvision.ops.nms(final_boxes, final_scores, nms_threshold)\n",
    "                 final_boxes = final_boxes[nms_indices]\n",
    "                 final_scores = final_scores[nms_indices]\n",
    "                 final_labels = final_labels[nms_indices]\n",
    "\n",
    "            batch_preds.append({'boxes': final_boxes.cpu(), 'scores': final_scores.cpu(), 'labels': final_labels.cpu().to(torch.int64)})\n",
    "\n",
    "        all_preds.extend(batch_preds)\n",
    "        for target in targets: all_targets_for_metric.append({'boxes': target['boxes'].cpu(), 'labels': target['labels'].cpu().to(torch.int64)})\n",
    "\n",
    "    return all_preds, all_targets_for_metric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53597a71",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7fe46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mAP(predictions, ground_truths):\n",
    "    if not predictions or not ground_truths: print(\"Eval failed: No preds/GTs.\"); return None\n",
    "    min_len = min(len(predictions), len(ground_truths))\n",
    "    if len(predictions) != len(ground_truths): print(f\"Eval Warning: Pred/GT mismatch ({len(predictions)} vs {len(ground_truths)}). Using {min_len} samples.\")\n",
    "    if min_len == 0: return None\n",
    "    predictions, ground_truths = predictions[:min_len], ground_truths[:min_len]\n",
    "\n",
    "    metric = torchmetrics.detection.MeanAveragePrecision(iou_type=\"bbox\", class_metrics=True).to(DEVICE) # Send metric to device\n",
    "    formatted_preds, formatted_gts = [], []\n",
    "    for p in predictions:\n",
    "         if isinstance(p, dict) and 'boxes' in p and 'scores' in p and 'labels' in p:\n",
    "              formatted_preds.append({k: torch.as_tensor(v).to(DEVICE) for k, v in p.items()}) # Send preds to device\n",
    "    for gt in ground_truths:\n",
    "         if isinstance(gt, dict) and 'boxes' in gt and 'labels' in gt:\n",
    "              formatted_gts.append({k: torch.as_tensor(v).to(DEVICE) for k, v in gt.items()}) # Send GTs to device\n",
    "\n",
    "    if len(formatted_preds) != len(formatted_gts): print(\"Eval failed: Mismatch after formatting.\"); return None\n",
    "    if not formatted_preds: print(\"Eval failed: No valid formatted data.\"); return None\n",
    "\n",
    "    try:\n",
    "        metric.update(formatted_preds, formatted_gts)\n",
    "        computed_metrics = metric.compute() # Compute on device\n",
    "        # Move results back to CPU for printing/logging if needed\n",
    "        computed_metrics = {k: v.cpu() if isinstance(v, torch.Tensor) else v for k, v in computed_metrics.items()}\n",
    "        print(\"\\nEvaluation Metrics:\")\n",
    "        # ... [Rest of the printing logic remains the same, using INV_CLASS_MAP] ...\n",
    "        for key, value in computed_metrics.items():\n",
    "            if isinstance(value, torch.Tensor) and value.numel() == 1:\n",
    "                 print(f\"  {key}: {value.item():.4f}\")\n",
    "            elif isinstance(value, list) and key == 'map_per_class':\n",
    "                print(f\"  {key}:\")\n",
    "                metric_classes = computed_metrics.get('classes', [])\n",
    "                if isinstance(metric_classes, torch.Tensor): metric_classes = [c.item() for c in metric_classes]\n",
    "                for i, ap in enumerate(value):\n",
    "                    if i < len(metric_classes):\n",
    "                         class_id = metric_classes[i]\n",
    "                         class_name = INV_CLASS_MAP.get(class_id, f\"Unknown_ID_{class_id}\")\n",
    "                         if ap != -1: print(f\"    - {class_name} (ID: {class_id}): {ap.item():.4f}\")\n",
    "                    else: print(f\"    - Index {i} (Class ID unknown): {ap.item():.4f}\")\n",
    "            elif isinstance(value, torch.Tensor) and key == 'classes':\n",
    "                 class_ids = [c.item() for c in value]\n",
    "                 class_names = [INV_CLASS_MAP.get(cid, f\"ID_{cid}\") for cid in class_ids]\n",
    "                 print(f\"  Classes Evaluated (IDs): {class_ids}\")\n",
    "                 print(f\"  Classes Evaluated (Names): {class_names}\")\n",
    "            else: print(f\"  {key}: {value}\")\n",
    "        return computed_metrics\n",
    "    except Exception as e: print(f\"Error computing mAP metrics: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93064d1",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a5f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(f\"Starting Few-Shot YOLO (yolov11n) Detection - {NUM_CLASSES} Classes, {N_SHOTS}-Shot...\")\n",
    "    print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "    # 1. Load Base YOLO Model\n",
    "    print(f\"\\nLoading base YOLO model from: {YOLO_MODEL_PATH}\")\n",
    "    try:\n",
    "        full_yolo_model = YOLO(YOLO_MODEL_PATH)\n",
    "        full_yolo_model.to(DEVICE)\n",
    "        print(\"Base YOLO model loaded successfully.\")\n",
    "        # --- Uncomment to Inspect model structure ---\n",
    "        print(\"\\nInspecting model structure (adjust FEATURE_LAYER_INDEX if needed):\")\n",
    "        try: print(full_yolo_model.model.model)\n",
    "        except AttributeError: print(\"Could not access default model structure.\")\n",
    "        print(\"\\n\")\n",
    "        print(f\"Using feature layer index: {FEATURE_LAYER_INDEX}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YOLO model '{YOLO_MODEL_PATH}': {e}\")\n",
    "        exit()\n",
    "\n",
    "    # 2. Create Feature Extractor Wrapper\n",
    "    try:\n",
    "        feature_extractor = YOLOFeatureExtractor(full_yolo_model, FEATURE_LAYER_INDEX).to(DEVICE)\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating feature extractor: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # 3. Prepare Data\n",
    "    target_img_size = (640, 640)\n",
    "    transforms = get_transform(target_size=target_img_size)\n",
    "    try:\n",
    "        full_train_dataset = RareDiseaseDataset(TRAIN_IMG_DIR, TRAIN_LABEL_DIR, TARGET_CLASS_IDS, transforms, target_img_size)\n",
    "        if len(full_train_dataset) == 0: raise ValueError(\"No training images loaded.\")\n",
    "        # Perform Train/Val Split\n",
    "        # (Split logic remains the same as previous response)\n",
    "        dataset_size = len(full_train_dataset)\n",
    "        indices = list(range(dataset_size))\n",
    "        min_required_train = N_SHOTS * NUM_CLASSES\n",
    "        can_split = dataset_size > min_required_train # Check if we have more images than needed for support\n",
    "\n",
    "        if can_split and VALIDATION_SPLIT > 0:\n",
    "            split = int(np.floor(VALIDATION_SPLIT * dataset_size))\n",
    "            # Ensure validation set is at least 1 if possible, and train set is large enough\n",
    "            split = max(1, split) if dataset_size > 1 else 0\n",
    "            if dataset_size - split < min_required_train:\n",
    "                print(f\"Warning: Validation split ({split}) leaves too few images ({dataset_size - split}) for {N_SHOTS}-shot training. Disabling validation.\")\n",
    "                split = 0 # Disable validation split\n",
    "\n",
    "            np.random.shuffle(indices)\n",
    "            train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "        else: # Cannot split or validation disabled\n",
    "             print(\"Warning: Dataset too small or validation split is 0. Using all data for training, no validation.\")\n",
    "             train_indices = indices\n",
    "             val_indices = []\n",
    "\n",
    "        train_subset_indices = train_indices\n",
    "        val_subset = Subset(full_train_dataset, val_indices) if val_indices else None\n",
    "        print(f\"Total train samples available: {len(train_subset_indices)}, Val samples: {len(val_subset) if val_subset else 0}\")\n",
    "\n",
    "\n",
    "        # Select Support Set\n",
    "        img_ids_by_class = full_train_dataset.get_image_ids_by_class() # This will raise ValueError if insufficient data\n",
    "\n",
    "        support_indices = []\n",
    "        print(f\"\\nSelecting {N_SHOTS}-shot support set from training split...\")\n",
    "        possible_support_indices = {cls_id: [idx for idx in ids if idx in train_subset_indices]\n",
    "                                    for cls_id, ids in img_ids_by_class.items()}\n",
    "\n",
    "        for cls_id in TARGET_CLASS_IDS:\n",
    "            available_indices = possible_support_indices.get(cls_id, [])\n",
    "            # Check against N_SHOTS (get_image_ids_by_class already did, but double-check within split)\n",
    "            if len(available_indices) < N_SHOTS:\n",
    "                raise ValueError(f\"Insufficient images ({len(available_indices)}) for class {INV_CLASS_MAP.get(cls_id, cls_id)} (ID: {cls_id}) within the training split after validation split.\")\n",
    "            support_indices.extend(random.sample(available_indices, N_SHOTS))\n",
    "        support_indices = sorted(list(set(support_indices)))\n",
    "        print(f\"Total unique support images selected: {len(support_indices)}\")\n",
    "\n",
    "        support_dataset = Subset(full_train_dataset, support_indices)\n",
    "        support_dataloader = DataLoader(support_dataset, batch_size=max(1, BATCH_SIZE // 2), shuffle=False, collate_fn=collate_fn, num_workers=2)\n",
    "\n",
    "        # Query Set\n",
    "        query_indices = [idx for idx in train_subset_indices if idx not in support_indices]\n",
    "        if not query_indices and len(train_subset_indices) > 0:\n",
    "            print(\"Warning: No query images left. Using support set also as query set.\")\n",
    "            query_indices = support_indices\n",
    "        elif not query_indices:\n",
    "            raise ValueError(\"Error: No training images available for query set.\")\n",
    "\n",
    "        query_dataset = Subset(full_train_dataset, query_indices)\n",
    "        query_dataloader = DataLoader(query_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
    "        print(f\"Support set size: {len(support_dataset)}, Query set size: {len(query_dataset)}\")\n",
    "\n",
    "        # Validation Loader\n",
    "        val_dataloader = DataLoader(val_subset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2) if val_subset else None\n",
    "\n",
    "        # Test Loader\n",
    "        test_dataset = RareDiseaseDataset(TEST_IMG_DIR, TEST_LABEL_DIR, TARGET_CLASS_IDS, transforms, target_img_size)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn, num_workers=2) if len(test_dataset) > 0 else None\n",
    "        print(f\"Test set size: {len(test_dataset)}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during data loading or splitting: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        exit()\n",
    "\n",
    "    # 4. Initialize RoIAlign\n",
    "    roi_align = RoIAlign(output_size=ROI_OUTPUT_SIZE, spatial_scale=1.0, sampling_ratio=-1).to(DEVICE)\n",
    "\n",
    "    # 5. Calculate Initial Prototypes\n",
    "    try:\n",
    "        prototypes = calculate_prototypes(support_dataloader, feature_extractor, roi_align, TARGET_CLASS_IDS, DEVICE)\n",
    "        if len(prototypes) < NUM_CLASSES:\n",
    "            print(f\"WARNING: Only generated prototypes for {len(prototypes)} out of {NUM_CLASSES} classes. Training/Inference might be affected.\")\n",
    "            if not prototypes: raise ValueError(\"No prototypes generated.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating initial prototypes: {e}\")\n",
    "        exit()\n",
    "\n",
    "    # 6. Setup Fine-tuning\n",
    "    optimizer = None\n",
    "    scheduler = None\n",
    "    if FINETUNE_FEATURE_EXTRACTOR:\n",
    "        params_to_tune = list(feature_extractor.parameters())\n",
    "        print(f\"Setting up optimizer for {len(params_to_tune)} parameters...\")\n",
    "        if not params_to_tune: FINETUNE_FEATURE_EXTRACTOR = False; print(\"No params found, disabling fine-tuning.\")\n",
    "        else:\n",
    "            optimizer = torch.optim.AdamW(params_to_tune, lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "    if not FINETUNE_FEATURE_EXTRACTOR: print(\"Fine-tuning disabled.\")\n",
    "\n",
    "    # 7. Training Loop\n",
    "    # (Loop logic remains the same as previous response)\n",
    "    print(\"\\nStarting Training/Adaptation Phase...\")\n",
    "    best_val_map = -1.0\n",
    "    try:\n",
    "        for epoch in range(NUM_EPOCHS):\n",
    "            if FINETUNE_FEATURE_EXTRACTOR and epoch > 0 and epoch % PROTO_UPDATE_FREQ == 0:\n",
    "                print(f\"\\n--- Recalculating Prototypes (Epoch {epoch+1}) ---\")\n",
    "                current_prototypes = calculate_prototypes(support_dataloader, feature_extractor, roi_align, TARGET_CLASS_IDS, DEVICE)\n",
    "                if len(current_prototypes) == len(prototypes): prototypes = current_prototypes # Update only if all protos recalculate successfully\n",
    "                else: print(\"Warning: Failed to recalculate all prototypes. Keeping previous.\")\n",
    "\n",
    "            epoch_loss = train_few_shot_yolo(\n",
    "                feature_extractor, prototypes, optimizer, scheduler, query_dataloader,\n",
    "                roi_align, DEVICE, epoch, FINETUNE_FEATURE_EXTRACTOR\n",
    "            )\n",
    "\n",
    "            if val_dataloader:\n",
    "                print(f\"\\n--- Running Validation after Epoch {epoch+1} ---\")\n",
    "                val_preds, val_gts = inference_yolo_few_shot(\n",
    "                    full_yolo_model, feature_extractor, prototypes, val_dataloader,\n",
    "                    roi_align, DEVICE\n",
    "                )\n",
    "                val_metrics = evaluate_mAP(val_preds, val_gts)\n",
    "                current_map = -1.0\n",
    "                if val_metrics and 'map' in val_metrics:\n",
    "                    val_map_tensor = val_metrics['map']\n",
    "                    if isinstance(val_map_tensor, torch.Tensor) and val_map_tensor.numel() == 1: current_map = val_map_tensor.item()\n",
    "\n",
    "                if current_map != -1.0: # Check if mAP calculation was successful\n",
    "                    if current_map > best_val_map:\n",
    "                        best_val_map = current_map\n",
    "                        print(f\"*** New best validation mAP: {best_val_map:.4f}. Saving prototypes and model... ***\")\n",
    "                        os.makedirs(os.path.dirname(SAVE_PROTOTYPES_PATH), exist_ok=True)\n",
    "                        torch.save({cls_id: p.cpu() for cls_id, p in prototypes.items()}, SAVE_PROTOTYPES_PATH)\n",
    "                        if FINETUNE_FEATURE_EXTRACTOR: torch.save(full_yolo_model.state_dict(), SAVE_FINETUNED_MODEL_PATH)\n",
    "                    else: print(f\"Validation mAP: {current_map:.4f} (Best: {best_val_map:.4f})\")\n",
    "                else: print(\"Validation mAP could not be calculated.\")\n",
    "                print(\"-------------------------------------------\\n\")\n",
    "            # Saving logic if no validation remains same\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"\\nAn error occurred during training loop: {e}\")\n",
    "         import traceback\n",
    "         traceback.print_exc()\n",
    "         print(\"Attempting to proceed to final evaluation...\")\n",
    "\n",
    "    print(\"Training/Adaptation finished.\")\n",
    "\n",
    "    # 8. Final Evaluation\n",
    "    if test_dataloader:\n",
    "        print(\"\\n--- Running Final Evaluation on Test Set ---\")\n",
    "        best_proto_path = SAVE_PROTOTYPES_PATH if os.path.exists(SAVE_PROTOTYPES_PATH) else f\"{os.path.splitext(SAVE_PROTOTYPES_PATH)[0]}_epoch{NUM_EPOCHS}.pt\" # Example fallback\n",
    "        best_model_path = SAVE_FINETUNED_MODEL_PATH if os.path.exists(SAVE_FINETUNED_MODEL_PATH) else f\"{os.path.splitext(SAVE_FINETUNED_MODEL_PATH)[0]}_epoch{NUM_EPOCHS}.pt\"\n",
    "\n",
    "        if os.path.exists(best_proto_path):\n",
    "             print(f\"Loading best prototypes from: {best_proto_path}\")\n",
    "             loaded_protos_cpu = torch.load(best_proto_path, map_location='cpu')\n",
    "             prototypes = {cls_id: p.to(DEVICE) for cls_id, p in loaded_protos_cpu.items()}\n",
    "             print(f\"Loaded {len(prototypes)} prototypes.\")\n",
    "        else:\n",
    "             print(\"Warning: Best prototypes file not found. Using prototypes from last training state.\")\n",
    "             if 'prototypes' not in locals() or not prototypes: print(\"Error: No prototypes available for final evaluation.\"); exit()\n",
    "\n",
    "        if FINETUNE_FEATURE_EXTRACTOR and os.path.exists(best_model_path):\n",
    "             print(f\"Loading fine-tuned model state from: {best_model_path}\")\n",
    "             try:\n",
    "                full_yolo_model = YOLO(YOLO_MODEL_PATH) # Re-init architecture\n",
    "                full_yolo_model.load_state_dict(torch.load(best_model_path, map_location=DEVICE))\n",
    "                full_yolo_model.to(DEVICE)\n",
    "                feature_extractor = YOLOFeatureExtractor(full_yolo_model, FEATURE_LAYER_INDEX).to(DEVICE) # Re-wrap\n",
    "                print(\"Fine-tuned model loaded.\")\n",
    "             except Exception as e:\n",
    "                 print(f\"Error loading fine-tuned model state dict: {e}. Using last state from training.\")\n",
    "                 # Ensure feature_extractor still points to the model used in training\n",
    "                 if 'feature_extractor' not in locals():\n",
    "                     print(\"Error: Feature extractor not available.\")\n",
    "                     exit()\n",
    "\n",
    "        elif FINETUNE_FEATURE_EXTRACTOR: print(\"Warning: Fine-tuned model file not found. Using last state.\")\n",
    "        else: print(\"Using original pre-trained YOLO model features.\")\n",
    "\n",
    "        try:\n",
    "            test_preds, test_gts = inference_yolo_few_shot(\n",
    "                full_yolo_model, feature_extractor, prototypes, test_dataloader,\n",
    "                roi_align, DEVICE\n",
    "            )\n",
    "            print(\"\\n--- Final Test Set Performance ---\")\n",
    "            evaluate_mAP(test_preds, test_gts)\n",
    "        except Exception as e:\n",
    "             print(f\"An error occurred during final evaluation: {e}\")\n",
    "             import traceback\n",
    "             traceback.print_exc()\n",
    "    else: print(\"\\nSkipping final evaluation as no test data was loaded.\")\n",
    "\n",
    "    print(\"\\nScript finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "few_shot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
