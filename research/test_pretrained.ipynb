{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "gK5aUJ5A3dXu"
      },
      "outputs": [],
      "source": [
        "# Model Trainer\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from typing import Dict, List, Tuple, Any\n",
        "\n",
        "# Utils\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
            "Collecting pandas\n",
            "  Downloading pandas-2.2.3-cp39-cp39-win_amd64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\hoang\\appdata\\roaming\\python\\python39\\site-packages (from pandas) (2.9.0.post0)\n",
            "Collecting pytz>=2020.1 (from pandas)\n",
            "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas)\n",
            "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\hoang\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading pandas-2.2.3-cp39-cp39-win_amd64.whl (11.6 MB)\n",
            "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.3/11.6 MB ? eta -:--:--\n",
            "   ----- ---------------------------------- 1.6/11.6 MB 5.6 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 2.9/11.6 MB 5.8 MB/s eta 0:00:02\n",
            "   -------------- ------------------------- 4.2/11.6 MB 6.0 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 5.5/11.6 MB 6.1 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 6.8/11.6 MB 6.2 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 7.3/11.6 MB 5.5 MB/s eta 0:00:01\n",
            "   ---------------------------- ----------- 8.4/11.6 MB 5.4 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 9.2/11.6 MB 5.2 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 9.7/11.6 MB 5.1 MB/s eta 0:00:01\n",
            "   ------------------------------------- -- 10.7/11.6 MB 4.9 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 11.3/11.6 MB 4.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 11.6/11.6 MB 4.8 MB/s eta 0:00:00\n",
            "Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
            "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
            "Installing collected packages: pytz, tzdata, pandas\n",
            "Successfully installed pandas-2.2.3 pytz-2025.1 tzdata-2025.1\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.comNote: you may need to restart the kernel to use updated packages.\n",
            "\n",
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages (from seaborn) (1.26.4)\n",
            "Requirement already satisfied: pandas>=1.2 in c:\\users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages (from seaborn) (2.2.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages (from seaborn) (3.9.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\hoang\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in c:\\users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hoang\\appdata\\roaming\\python\\python39\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (6.5.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\hoang\\appdata\\roaming\\python\\python39\\site-packages (from importlib-resources>=3.2.0->matplotlib!=3.6.1,>=3.4->seaborn) (3.21.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\hoang\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "Installing collected packages: seaborn\n",
            "Successfully installed seaborn-0.13.2\n"
          ]
        }
      ],
      "source": [
        "%pip install seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jj7Elisb3W_o"
      },
      "outputs": [],
      "source": [
        "class ModelTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        models_dict: Dict[str, nn.Module],\n",
        "        train_loader: DataLoader,\n",
        "        test_loader: DataLoader,\n",
        "        criterion: nn.Module,\n",
        "        num_epochs: int = 10,\n",
        "        device: str = None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize the model trainer with multiple models and dataset loaders.\n",
        "\n",
        "        Args:\n",
        "            models_dict: Dictionary of model names and their instances\n",
        "            train_loader: Training data loader\n",
        "            test_loader: Test data loader\n",
        "            criterion: Loss function\n",
        "            num_epochs: Number of training epochs\n",
        "            device: Device to run training on (will auto-detect if None)\n",
        "        \"\"\"\n",
        "        self.device = device or ('cpu')\n",
        "        self.models = {name: model.to(self.device) for name, model in models_dict.items()}\n",
        "        self.train_loader = train_loader\n",
        "        self.test_loader = test_loader\n",
        "        self.criterion = criterion\n",
        "        self.num_epochs = num_epochs\n",
        "        self.results = {}\n",
        "\n",
        "    def _validate_input_batch(self, inputs: torch.Tensor, labels: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Validate and prepare input batch for training/testing.\"\"\"\n",
        "        if not isinstance(inputs, torch.Tensor):\n",
        "            raise TypeError(f\"Expected inputs to be torch.Tensor, got {type(inputs)}\")\n",
        "        if not isinstance(labels, torch.Tensor):\n",
        "            raise TypeError(f\"Expected labels to be torch.Tensor, got {type(labels)}\")\n",
        "\n",
        "        return inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "    def train_model(self, model_name: str, learning_rate: float = 0.001) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Train a single model and track its performance metrics.\n",
        "\n",
        "        Args:\n",
        "            model_name: Name of the model to train\n",
        "            learning_rate: Learning rate for optimization\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing training history\n",
        "        \"\"\"\n",
        "        if model_name not in self.models:\n",
        "            raise ValueError(f\"Model {model_name} not found in initialized models\")\n",
        "\n",
        "        model = self.models[model_name]\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        history = {\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_metrics': None\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            for epoch in range(self.num_epochs):\n",
        "                # Training phase\n",
        "                model.train()\n",
        "                train_loss = 0\n",
        "                correct = 0\n",
        "                total = 0\n",
        "\n",
        "                for batch_idx, (inputs, labels) in enumerate(self.train_loader):\n",
        "                    try:\n",
        "                        inputs, labels = self._validate_input_batch(inputs, labels)\n",
        "\n",
        "                        optimizer.zero_grad()\n",
        "                        outputs = model(inputs)\n",
        "                        loss = self.criterion(outputs, labels)\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        train_loss += loss.item()\n",
        "                        _, predicted = outputs.max(1)\n",
        "                        total += labels.size(0)\n",
        "                        correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                    except RuntimeError as e:\n",
        "                        print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "                train_acc = 100. * correct / total\n",
        "                train_loss = train_loss / len(self.train_loader)\n",
        "\n",
        "                # Validation phase\n",
        "                val_loss, val_acc = self._validate_epoch(model)\n",
        "\n",
        "                # Save metrics\n",
        "                history['train_loss'].append(train_loss)\n",
        "                history['train_acc'].append(train_acc)\n",
        "                history['val_loss'].append(val_loss)\n",
        "                history['val_acc'].append(val_acc)\n",
        "\n",
        "                print(f'Epoch [{epoch+1}/{self.num_epochs}] - '\n",
        "                      f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, '\n",
        "                      f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Training interrupted for {model_name}: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "        # Final test phase\n",
        "        history['test_metrics'] = self.test(model)\n",
        "        self.results[model_name] = history\n",
        "        return history\n",
        "\n",
        "    def _validate_epoch(self, model: nn.Module) -> Tuple[float, float]:\n",
        "        \"\"\"Run validation for one epoch.\"\"\"\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in self.test_loader:\n",
        "                inputs, labels = self._validate_input_batch(inputs, labels)\n",
        "                outputs = model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "        return val_loss / len(self.test_loader), 100. * correct / total\n",
        "\n",
        "    def test(self, model: nn.Module) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Test model and return metrics\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing test metrics\n",
        "        \"\"\"\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        predictions = []\n",
        "        targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, (inputs, labels) in enumerate(self.test_loader):\n",
        "                try:\n",
        "                    inputs, labels = self._validate_input_batch(inputs, labels)\n",
        "\n",
        "                    outputs = model(inputs)\n",
        "                    loss = self.criterion(outputs, labels)\n",
        "                    test_loss += loss.item()\n",
        "\n",
        "                    _, predicted = outputs.max(1)  # Multi-class case\n",
        "                    predictions.append(predicted.cpu().numpy())\n",
        "                    targets.append(labels.cpu().numpy())\n",
        "\n",
        "                    total += labels.size(0)\n",
        "                    correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                except RuntimeError as e:\n",
        "                    print(f\"Error in batch {batch_idx}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        # Flatten predictions and targets for metrics\n",
        "        all_predictions = np.concatenate(predictions)\n",
        "        all_targets = np.concatenate(targets)\n",
        "\n",
        "        test_acc = 100. * correct / total\n",
        "        return {\n",
        "            'test_loss': test_loss / len(self.test_loader),\n",
        "            'test_accuracy': test_acc,\n",
        "            'predictions': all_predictions,\n",
        "            'targets': all_targets\n",
        "        }\n",
        "\n",
        "    def train_all_models(self) -> None:\n",
        "        \"\"\"Train all models and save results\"\"\"\n",
        "        for model_name in self.models:\n",
        "            try:\n",
        "                self.train_model(model_name)\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to train {model_name}: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        self.save_results()\n",
        "        self.generate_reports()\n",
        "\n",
        "    def save_results(self) -> None:\n",
        "        \"\"\"Save training results to CSV\"\"\"\n",
        "        results_df = pd.DataFrame()\n",
        "\n",
        "        for model_name, history in self.results.items():\n",
        "            model_df = pd.DataFrame({\n",
        "                'epoch': range(1, self.num_epochs + 1),\n",
        "                'model': model_name,\n",
        "                'train_loss': history['train_loss'],\n",
        "                'train_acc': history['train_acc'],\n",
        "                'val_loss': history['val_loss'],\n",
        "                'val_acc': history['val_acc']\n",
        "            })\n",
        "            results_df = pd.concat([results_df, model_df])\n",
        "\n",
        "        os.makedirs('results', exist_ok=True)\n",
        "        results_df.to_csv('results/training_results.csv', index=False)\n",
        "\n",
        "    def generate_reports(self) -> None:\n",
        "        \"\"\"Generate and save visualization plots\"\"\"\n",
        "        os.makedirs('plots', exist_ok=True)\n",
        "\n",
        "        self._plot_training_curves()\n",
        "        self._plot_confusion_matrices()\n",
        "        self._plot_model_comparison()\n",
        "        self._analyze_fitting()\n",
        "\n",
        "    def _plot_training_curves(self) -> None:\n",
        "        \"\"\"Plot training and validation curves for all models\"\"\"\n",
        "        plt.figure(figsize=(15, 10))\n",
        "\n",
        "        for model_name, history in self.results.items():\n",
        "            epochs = range(1, self.num_epochs + 1)\n",
        "\n",
        "            # Loss subplot\n",
        "            plt.subplot(2, 1, 1)\n",
        "            plt.plot(epochs, history['train_loss'], '-o', label=f'{model_name} (train)')\n",
        "            plt.plot(epochs, history['val_loss'], '--o', label=f'{model_name} (val)')\n",
        "            plt.title('Model Loss')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Loss')\n",
        "            plt.legend()\n",
        "\n",
        "            # Accuracy subplot\n",
        "            plt.subplot(2, 1, 2)\n",
        "            plt.plot(epochs, history['train_acc'], '-o', label=f'{model_name} (train)')\n",
        "            plt.plot(epochs, history['val_acc'], '--o', label=f'{model_name} (val)')\n",
        "            plt.title('Model Accuracy')\n",
        "            plt.xlabel('Epoch')\n",
        "            plt.ylabel('Accuracy (%)')\n",
        "            plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('plots/training_curves.png')\n",
        "        plt.close()\n",
        "\n",
        "    def _plot_confusion_matrices(self) -> None:\n",
        "        \"\"\"Plot confusion matrices for all models\"\"\"\n",
        "        for model_name, history in self.results.items():\n",
        "            metrics = history['test_metrics']\n",
        "            cm = confusion_matrix(metrics['targets'], metrics['predictions'])\n",
        "\n",
        "            plt.figure(figsize=(10, 8))\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "            plt.title(f'Confusion Matrix - {model_name}')\n",
        "            plt.xlabel('Predicted')\n",
        "            plt.ylabel('True')\n",
        "            plt.savefig(f'plots/confusion_matrix_{model_name}.png')\n",
        "            plt.close()\n",
        "\n",
        "    def _plot_model_comparison(self) -> None:\n",
        "        \"\"\"Plot final test accuracy comparison\"\"\"\n",
        "        model_names = list(self.results.keys())\n",
        "        test_accuracies = [self.results[model]['test_metrics']['test_accuracy']\n",
        "                          for model in model_names]\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        bars = plt.bar(model_names, test_accuracies)\n",
        "        plt.title('Model Comparison - Test Accuracy')\n",
        "        plt.xlabel('Model')\n",
        "        plt.ylabel('Test Accuracy (%)')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Add value labels on top of each bar\n",
        "        for bar in bars:\n",
        "            height = bar.get_height()\n",
        "            plt.text(bar.get_x() + bar.get_width()/2., height,\n",
        "                    f'{height:.1f}%',\n",
        "                    ha='center', va='bottom')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('plots/model_comparison.png')\n",
        "        plt.close()\n",
        "\n",
        "    def _analyze_fitting(self) -> None:\n",
        "        \"\"\"Analyze and report fitting status for each model\"\"\"\n",
        "        fitting_analysis = {}\n",
        "\n",
        "        for model_name, history in self.results.items():\n",
        "            train_loss = history['train_loss']\n",
        "            val_loss = history['val_loss']\n",
        "\n",
        "            # Calculate metrics for fitting analysis\n",
        "            final_train_loss = train_loss[-1]\n",
        "            final_val_loss = val_loss[-1]\n",
        "            loss_gap = final_val_loss - final_train_loss\n",
        "\n",
        "            # Determine fitting status\n",
        "            if final_train_loss > 0.1 and final_val_loss > 0.1:\n",
        "                status = \"Underfitting\"\n",
        "            elif loss_gap > 0.1:\n",
        "                status = \"Overfitting\"\n",
        "            else:\n",
        "                status =  \"Fit\"\n",
        "\n",
        "            fitting_analysis[model_name] = {\n",
        "                'status': status,\n",
        "                'final_train_loss': final_train_loss,\n",
        "                'final_val_loss': final_val_loss,\n",
        "                'loss_gap': loss_gap,\n",
        "                'recommended_action': self._get_fitting_recommendation(status)\n",
        "            }\n",
        "\n",
        "        with open('results/fitting_analysis.json', 'w') as f:\n",
        "            json.dump(fitting_analysis, f, indent=4)\n",
        "\n",
        "    def _get_fitting_recommendation(self, status: str) -> str:\n",
        "        \"\"\"Get recommendation based on fitting status\"\"\"\n",
        "        recommendations = {\n",
        "            \"Underfitting\": \"Consider increasing model capacity or training longer\",\n",
        "            \"Overfitting\": \"Consider adding regularization or reducing model capacity\",\n",
        "            \"Fit\": \"Model is well-balanced, continue monitoring performance\"\n",
        "        }\n",
        "        return recommendations.get(status, \"Unknown fitting status\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "d7XatJoR3tEn"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[WinError 3] The system cannot find the path specified: 'train'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[14], line 10\u001b[0m\n\u001b[0;32m      3\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[0;32m      4\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[0;32m      5\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mToTensor(),\n\u001b[0;32m      6\u001b[0m     transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m], std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m]),\n\u001b[0;32m      7\u001b[0m ])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Load train dataset\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mImageFolder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Load test dataset\u001b[39;00m\n\u001b[0;32m     13\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mImageFolder(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m, transform\u001b[38;5;241m=\u001b[39mtransform)\n",
            "File \u001b[1;32mc:\\Users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages\\torchvision\\datasets\\folder.py:328\u001b[0m, in \u001b[0;36mImageFolder.__init__\u001b[1;34m(self, root, transform, target_transform, loader, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    321\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    326\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    327\u001b[0m ):\n\u001b[1;32m--> 328\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mIMG_EXTENSIONS\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    333\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtarget_transform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_transform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_valid_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_valid_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_empty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_empty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples\n",
            "File \u001b[1;32mc:\\Users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages\\torchvision\\datasets\\folder.py:149\u001b[0m, in \u001b[0;36mDatasetFolder.__init__\u001b[1;34m(self, root, loader, extensions, transform, target_transform, is_valid_file, allow_empty)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    140\u001b[0m     root: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    146\u001b[0m     allow_empty: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    147\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(root, transform\u001b[38;5;241m=\u001b[39mtransform, target_transform\u001b[38;5;241m=\u001b[39mtarget_transform)\n\u001b[1;32m--> 149\u001b[0m     classes, class_to_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_dataset(\n\u001b[0;32m    151\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot,\n\u001b[0;32m    152\u001b[0m         class_to_idx\u001b[38;5;241m=\u001b[39mclass_to_idx,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    155\u001b[0m         allow_empty\u001b[38;5;241m=\u001b[39mallow_empty,\n\u001b[0;32m    156\u001b[0m     )\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader \u001b[38;5;241m=\u001b[39m loader\n",
            "File \u001b[1;32mc:\\Users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages\\torchvision\\datasets\\folder.py:234\u001b[0m, in \u001b[0;36mDatasetFolder.find_classes\u001b[1;34m(self, directory)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_classes\u001b[39m(\u001b[38;5;28mself\u001b[39m, directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m    208\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Find the class folders in a dataset structured as follows::\u001b[39;00m\n\u001b[0;32m    209\u001b[0m \n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m        directory/\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;124;03m        (Tuple[List[str], Dict[str, int]]): List of all classes and dictionary mapping each class to an index.\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfind_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\hoang\\anaconda3\\envs\\coffe_leaf_project\\lib\\site-packages\\torchvision\\datasets\\folder.py:41\u001b[0m, in \u001b[0;36mfind_classes\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_classes\u001b[39m(directory: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[List[\u001b[38;5;28mstr\u001b[39m], Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mint\u001b[39m]]:\n\u001b[0;32m     37\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Finds the class folders in a dataset.\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \u001b[38;5;124;03m    See :class:`DatasetFolder` for details.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m     classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(entry\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscandir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m entry\u001b[38;5;241m.\u001b[39mis_dir())\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m classes:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find any class folder in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdirectory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'train'"
          ]
        }
      ],
      "source": [
        "from torchvision import transforms, models, datasets\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Load train dataset\n",
        "train_dataset = datasets.ImageFolder('train', transform=transform)\n",
        "\n",
        "# Load test dataset\n",
        "test_dataset = datasets.ImageFolder('test', transform=transform)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "luZ6MwNR32B-",
        "outputId": "b5f71df6-cf1f-4404-bd76-74a187cd7c97"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[0;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m num_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241m.\u001b[39mclasses)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(num_labels)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mEfficientNet\u001b[39m():\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "num_labels = len(train_dataset.classes)\n",
        "print(num_labels)\n",
        "\n",
        "\n",
        "def EfficientNet():\n",
        "    model = models.efficientnet_b0(pretrained=True)\n",
        "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_labels)\n",
        "    return model.to(device)\n",
        "\n",
        "def ResNet50():\n",
        "    model = models.resnet50(pretrained=True)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_labels)\n",
        "    return model.to(device)\n",
        "\n",
        "def VGG16():\n",
        "    model = models.vgg16(pretrained=True)\n",
        "    model.classifier[6] = nn.Linear(model.classifier[6].in_features, num_labels)\n",
        "    return model.to(device)\n",
        "\n",
        "def MobileNet():\n",
        "    model = models.mobilenet_v2(pretrained=True)\n",
        "    model.classifier[1] = nn.Linear(model.classifier[1].in_features, num_labels)\n",
        "    return model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zhRc3ZG33_P",
        "outputId": "2e17fc37-3071-4a74-ca1a-10a5810f4b05"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "models = {\n",
        "    \"EfficientNet\": EfficientNet(),\n",
        "    \"ResNet50\": ResNet50(),\n",
        "    \"VGG16\": VGG16(),\n",
        "    \"MobileNet\": MobileNet()\n",
        "}\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t849qh5V3_vO",
        "outputId": "52520d35-eeb5-4049-cc1c-7e3041f2e4e3"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "trainer = ModelTrainer(models, train_loader, test_loader, criterion, num_epochs=3)\n",
        "trainer.train_all_models()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4pyIXBSFt0O",
        "outputId": "da32cb2e-066a-42fb-8c8c-de8b5eee059d"
      },
      "outputs": [],
      "source": [
        "trainer.test(models['EfficientNet'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IBbibRBF4-9",
        "outputId": "d203b58c-6632-4fd5-c810-17c79b25e358"
      },
      "outputs": [],
      "source": [
        "trainer.test(models['ResNet50'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZQXMT0GGB5d",
        "outputId": "3c5c6d99-85f5-488b-bee9-995b8506c8df"
      },
      "outputs": [],
      "source": [
        "trainer.test(models['VGG16'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jIPdpLqeGGTO",
        "outputId": "cabd6a1a-bca0-4aa3-8651-1938385e7f84"
      },
      "outputs": [],
      "source": [
        "trainer.test(models['MobileNet'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k-_MJVxpGQ3Q"
      },
      "outputs": [],
      "source": [
        "trainer._plot_training_curves()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HfOT2ufcBqWA"
      },
      "outputs": [],
      "source": [
        "trainer.save_results()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZ5bQTHsCJdY"
      },
      "outputs": [],
      "source": [
        "trainer.generate_reports()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "coffe_leaf_project",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
